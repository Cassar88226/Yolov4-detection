{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "Windows or Linux<br>\n",
    "CMake >= 3.12: https://cmake.org/download/<br>\n",
    "CUDA >= 10.0: https://developer.nvidia.com/cuda-toolkit-archive (on Linux do Post-installation Actions)<br>\n",
    "OpenCV >= 2.4: use your preferred package manager (brew, apt), build from source using vcpkg or download from OpenCV official site (on Windows set system variable OpenCV_DIR = C:\\opencv\\build - where are the include and x64 folders image)\n",
    "cuDNN >= 7.0 https://developer.nvidia.com/rdp/cudnn-archive (on Linux copy cudnn.h,libcudnn.so... as desribed here \n",
    "https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar , on Windows copy cudnn.h,cudnn64_7.dll, cudnn64_7.lib as desribed here https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows )<br>\n",
    "GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported<br>\n",
    "on Linux GCC or Clang, on Windows MSVC 2017/2019 https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Cloning and Building Darknet\n",
    "The following cells will clone darknet from AlexeyAB's famous repository, adjust the Makefile to enable OPENCV and GPU for darknet and then build darknet.\n",
    "\n",
    "Do not worry about any warnings when you run the '!make' cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify CUDA\n",
    "# !nvcc --version\n",
    "!/usr/local/cuda/bin/nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone Yolov4 darknet repo\n",
    "!git clone https://github.com/AlexeyAB/darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change makefile to have GPU and OPENCV enabled\n",
    "%cd darknet\n",
    "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
    "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
    "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
    "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make darknet (builds darknet so that you can then use the darknet executable file to run or train object detectors)\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Download pre-trained YOLOv4 weights\n",
    "YOLOv4 has been trained already on the coco dataset which has 80 classes that it can predict. We will grab these pretrained weights so that we can run YOLOv4 on these pretrained classes and get detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Download COCO Dataset of special categories\n",
    "categories are person, car, bus, truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import io\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download coco dataset annotation zip file\n",
    "%cd data\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip annotation zip file\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir coco\n",
    "!mkdir coco/obj\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current directory\n",
    "cdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(cdir, 'coco/obj')\n",
    "coco_path = os.path.join(cdir, 'coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO Bounding Box to Yolov4 Format\n",
    "# COCO Bounding Box format : x(left), y(top), width, height of object\n",
    "# [477.88, 87.8, 13.7, 33.4]\n",
    "# Yolov4 Format: central x, y of object, width, height of object\n",
    "# [0.757391, 0.244731, 0.021406, 0.07822]\n",
    "# rounding 6 decimal points\n",
    "def convertBbox2YoloFormat(bbox, size):\n",
    "  width, height = size\n",
    "  x = round((bbox[0] + bbox[2]/2) / width, 6)\n",
    "  y = round((bbox[1] + bbox[3]/2) / height, 6)\n",
    "  w = round(bbox[2]/width, 6)\n",
    "  h = round(bbox[3]/height, 6)\n",
    "  return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = \"train.txt\"\n",
    "train_txt_path = os.path.join(coco_path, train_txt)\n",
    "valid_txt = \"valid.txt\"\n",
    "valid_txt_path = os.path.join(coco_path, valid_txt)\n",
    "trainfile = open(train_txt_path, 'w')\n",
    "validfile = open(valid_txt_path, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildCustomDatasetFromCOCO(annFile, description_file):\n",
    "    coco=COCO(annFile)\n",
    "    # get Category Ids\n",
    "    catNames = ['person', 'car', 'bus', 'truck']\n",
    "    catIds = coco.getCatIds(catNms=catNames)\n",
    "    # get Image Ids for 4 categories\n",
    "    imgIds = []\n",
    "    for catId in catIds:\n",
    "      sub_imgIds = coco.getImgIds(catIds=catId)\n",
    "      print(len(sub_imgIds))\n",
    "      imgIds += sub_imgIds\n",
    "    # get unique Image Ids\n",
    "    imgIds = list(set(imgIds))\n",
    "    \n",
    "    # loop Image Ids\n",
    "    # get Image Information form Coco dataset\n",
    "    for imgId in imgIds:\n",
    "        img_info = coco.loadImgs(ids = imgId)[0]\n",
    "        # Load Image and annotation\n",
    "        img = io.imread(img_info['coco_url'])\n",
    "        annIds = coco.getAnnIds(imgIds=img_info['id'], catIds=catIds, iscrowd=0)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "\n",
    "        # get file name, e.g. 000000262145.jpg\n",
    "        filename = img_info['file_name']\n",
    "\n",
    "        basename = os.path.splitext(filename)[0]\n",
    "        txtfile_path = os.path.join(dataset_path, basename + '.txt')\n",
    "        # write the image path in train.txt\n",
    "        # example\n",
    "        # coco/obj/000000262145.jpg\n",
    "        # coco/obj/000000262146.jpg\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        description_file.write(image_path + \"\\n\")\n",
    "\n",
    "        # download image in coco/obj folder\n",
    "        io.imsave(image_path, img)\n",
    "\n",
    "        # write the yolo format bounding box in image.txt file\n",
    "        size = (img_info['width'], img_info['height'])\n",
    "        txtfile = open(txtfile_path, 'w')\n",
    "        for i, ann in enumerate(anns):      \n",
    "          bbox = convertBbox2YoloFormat(ann['bbox'], size)\n",
    "          item_str = str(catIds.index(ann['category_id']))\n",
    "          bbox_str = \" \".join(str(entry) for entry in bbox)\n",
    "          item_str += \" \" + bbox_str\n",
    "          if i != len(anns) - 1:\n",
    "            txtfile.write(item_str + \"\\n\")\n",
    "          else:\n",
    "            txtfile.write(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize COCO api for instance annotations\n",
    "dataDir='.'\n",
    "dataType='train2017'\n",
    "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "print(annFile)\n",
    "BuildCustomDatasetFromCOCO(annFile, trainfile)\n",
    "dataType='val2017'\n",
    "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "BuildCustomDatasetFromCOCO(annFile, validfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Write Custom Training Config for YOLOv4\n",
    "<br>\n",
    "<font size=5>Modify following files</font><br>\n",
    "&emsp;&emsp;cfg/yolov4-custom.cfg<br>\n",
    "&emsp;&emsp;cfg/coco.data<br>\n",
    "&emsp;&emsp;data/coco.names<br>\n",
    "\n",
    "<font size=4>Modify data/coco.names with our own categories</font><br>\n",
    "&emsp;&emsp;person<br>\n",
    "&emsp;&emsp;car<br>\n",
    "&emsp;&emsp;bus<br>\n",
    "&emsp;&emsp;truck<br>\n",
    "\n",
    "<font size=4>Modify cfg/coco.data</font><br>\n",
    "&emsp;&emsp;classes= 4<br>\n",
    "&emsp;&emsp;train  = /content/darknet/data/coco/train.txt<br>\n",
    "&emsp;&emsp;valid  = /content/darknet/data/coco/valid.txt<br>\n",
    "&emsp;&emsp;names = /content/darknet/data/coco.names<br>\n",
    "&emsp;&emsp;backup = /content/darknet/backup<br>\n",
    "\n",
    "<font size=4>Modify cfg/yolov4-custom.cfg</font><br>\n",
    "&emsp;&emsp;line 20, max_batches = 8000(4*2000)<br>\n",
    "&emsp;&emsp;line 22, steps = 6400, 7200(0.8, 0.9*8000)<br>\n",
    "&emsp;&emsp;change yolo layer classes to 4(class number), line 970, 1058, 1146<br>\n",
    "&emsp;&emsp;change filters of convolution to 27((classes + 5)x3) immediately before each 3 yolo layers, line 963, 1051, 1139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train the Model with Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "!./darknet detector train cfg/coco.data cfg/yolov4-custom.cfg yolov4.conv.137\n",
    "# train with multiple GPU\n",
    "# ./darknet detector train cfg/coco.data cfg/yolov4-custom.cfg yolov4.conv.137 -gpus 0,1,2,3\n",
    "# If want to stop and restart training from a checkpoint:\n",
    "# ./darknet detector train cfg/coco.data cfg/yolov4-custom.cfg backup/yolov3.backup -gpus 0,1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:Infer Custom Objects with Saved YOLOv4 Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define utility function\n",
    "def imShow(path):\n",
    "  import matplotlib.pyplot as plt\n",
    "  %matplotlib inline\n",
    "\n",
    "  image = cv2.imread(path)\n",
    "  height, width = image.shape[:2]\n",
    "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "  fig = plt.gcf()\n",
    "  fig.set_size_inches(18, 10)\n",
    "  plt.axis(\"off\")\n",
    "  #plt.rcParams['figure.figsize'] = [10, 5]\n",
    "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the images with trained yolov4 model\n",
    "#test out our detector!\n",
    "img_path = 'test.jpg'\n",
    "!./darknet detect cfg/yolov4-custom.cfg backup/custom-yolov4-detector_last.weights {img_path} -dont-show\n",
    "imShow('predictions.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7:Download VOC Dataset of special categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download VOC 2007, 2012 dataset tar files from url\n",
    "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
    "!wget http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upzip tar file\n",
    "!tar xf VOCtrainval_06-Nov-2007.tar\n",
    "!tar xf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from os import listdir, getcwd\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset description and classes\n",
    "# in VOC Dataset, there isn't a class of truck, so we can use only 3 classes - person, car, bus\n",
    "sets=[('2012', 'train'), ('2012', 'val'), ('2007', 'train'), ('2007', 'val')]\n",
    "classes = [\"person\", \"car\", , \"bus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert VOC bounding box to yolo darknet format\n",
    "def ConvertVOCBbox2YoloFormat(box, size):\n",
    "    # get central point of object\n",
    "    x = (box[0] + box[1])/2.0\n",
    "    y = (box[2] + box[3])/2.0\n",
    "    # get width and height\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    dw = 1./size[0]\n",
    "    dh = 1./size[1]\n",
    "    # convert bbox to darknet format\n",
    "    x = x * dw\n",
    "    y = y * dh\n",
    "    w = w * dw\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Annotation Xml files from VOC Dataset and create the Yolo darkent text files\n",
    "def ConvertVOCAnns(year, image_id):\n",
    "    # read annotation xml file\n",
    "    in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id))\n",
    "    out_file = open('VOCdevkit/VOC%s/labels/%s.txt'%(year, image_id), 'w')\n",
    "    # parse xml file\n",
    "    root = ET.parse(in_file)\n",
    "    # get size attribute\n",
    "    size = root.find('size')\n",
    "    # get width and height from size attrib\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "    size = (w, h)\n",
    "    # loop object attributes\n",
    "    for obj in root.iter('object'):\n",
    "        # get difficult and class\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        # filter object by class and difficult\n",
    "        if cls not in classes or int(difficult) == 1:\n",
    "            continue\n",
    "        # get class id and bound box\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        xmin = float(xmlbox.find('xmin').text)\n",
    "        ymin = float(xmlbox.find('ymin').text)\n",
    "        xmax = float(xmlbox.find('xmax').text)\n",
    "        ymax = float(xmlbox.find('ymax').text)\n",
    "        box = (xmin, xmax, ymin, ymax)\n",
    "        # convert bound box to darknet format\n",
    "        bbox = ConvertVOCBbox2YoloFormat(box, size)\n",
    "        # make darknet format annotation item\n",
    "        item_str = str(cls_id)\n",
    "        item_str += \" \" + \" \".join([str(e) for e in bbox])\n",
    "        item_str += \"\\n\"\n",
    "        # write item in annotation file\n",
    "        out_file.write(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current directory\n",
    "cdir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop all datasets\n",
    "# create the text and valid text file including image paths\n",
    "# this stage is the same as COCO dataset\n",
    "for year, image_set in sets:\n",
    "    if not os.path.exists('VOCdevkit/VOC%s/labels/'%(year)):\n",
    "        os.makedirs('VOCdevkit/VOC%s/labels/'%(year))\n",
    "    image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()\n",
    "    list_file = open('%s_%s.txt'%(year, image_set), 'w')                        \n",
    "    for image_id in image_ids:\n",
    "        list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\\n'%(cdir, year, image_id))\n",
    "        ConvertVOCAnns(year, image_id)\n",
    "    list_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Write Custom Configuration for COCO and VOC Dataset\n",
    "<br>\n",
    "If you need to train with COCO and VOC Dataset, we don't need to modify following files again<br>\n",
    "&emsp;&emsp;cfg/yolov4-custom.cfg<br>\n",
    "&emsp;&emsp;cfg/coco.data<br>\n",
    "&emsp;&emsp;data/coco.names<br>\n",
    "But you need to concatenate the train.txt and valid.txt files of COCO and VOC dataset.<br>\n",
    "And you can move the image files from VOC into the directory of COCO image files - in our case, darknet/data/obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Train the Model with COCO and VOC Dataset\n",
    "<br>\n",
    "This step is the same as # step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Download Open Images Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install OpenImage Dataset ToolKit from git\n",
    "!git clone https://github.com/theAIGuysCode/OIDv4_ToolKit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd OIDv4_ToolKit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images and annotations file form \n",
    "# arguments:\n",
    "#    --classes : list of classes\n",
    "#    --type_csv : 'train' or 'validation' or 'test' or 'all'\n",
    "#    --limit : integer number of each class\n",
    "!python main.py downloader --classes Person Car Bus Truck 'Vehicle registration plate' 'Human face' --type_csv train --multiclasses 1 --limit 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py downloader --classes Person Car Bus Truck 'Vehicle registration plate' 'Human face' --type_csv validation --multiclasses 1 --limit 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Modify the classes.txt file as following</font><br>\n",
    "\n",
    "&emsp;&emsp;Person<br>\n",
    "&emsp;&emsp;Car<br>\n",
    "&emsp;&emsp;Bus<br>\n",
    "&emsp;&emsp;Truck<br>\n",
    "&emsp;&emsp;Vehicle registration plate<br>\n",
    "&emsp;&emsp;Human face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert annotations text file to darknet format annotation text file\n",
    "# make the image list file for train,validation, test subset\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import fileinput\n",
    "\n",
    "# function that turns XMin, YMin, XMax, YMax coordinates to normalized yolo format\n",
    "def convert(filename_str, coords):\n",
    "    os.chdir(\"..\")\n",
    "    image = cv2.imread(filename_str + \".jpg\")\n",
    "    coords[2] -= coords[0]\n",
    "    coords[3] -= coords[1]\n",
    "    x_diff = int(coords[2]/2)\n",
    "    y_diff = int(coords[3]/2)\n",
    "    coords[0] = coords[0]+x_diff\n",
    "    coords[1] = coords[1]+y_diff\n",
    "    coords[0] /= int(image.shape[1])\n",
    "    coords[1] /= int(image.shape[0])\n",
    "    coords[2] /= int(image.shape[1])\n",
    "    coords[3] /= int(image.shape[0])\n",
    "    coords[0] = round(coords[0], 6)\n",
    "    coords[1] = round(coords[1], 6)\n",
    "    coords[2] = round(coords[2], 6)\n",
    "    coords[3] = round(coords[3], 6)\n",
    "    os.chdir(\"Label\")\n",
    "    return coords\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# create dict to map class names to numbers for yolo\n",
    "classes = {}\n",
    "with open(\"classes.txt\", \"r\") as myFile:\n",
    "    for num, line in enumerate(myFile, 0):\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        classes[line] = num\n",
    "    myFile.close()\n",
    "# step into dataset directory\n",
    "os.chdir(os.path.join(\"OID\", \"Dataset\"))\n",
    "DIRS = os.listdir(os.getcwd())\n",
    "\n",
    "train_file = open(\"train.txt\", \"w\")\n",
    "validation_file = open(\"validation.txt\", \"w\")\n",
    "test_file = open(\"test.txt\", \"w\")\n",
    "\n",
    "# for all train, validation and test folders\n",
    "for DIR in DIRS:\n",
    "    if os.path.isdir(DIR):\n",
    "        os.chdir(DIR)\n",
    "        print(\"Currently in subdirectory:\", DIR)\n",
    "        \n",
    "        CLASS_DIRS = os.listdir(os.getcwd())\n",
    "        # for all class folders step into directory to change annotations\n",
    "        for CLASS_DIR in CLASS_DIRS:\n",
    "            if os.path.isdir(CLASS_DIR):\n",
    "                os.chdir(CLASS_DIR)\n",
    "                print(\"Converting annotations for class: \", CLASS_DIR)\n",
    "\n",
    "                for filename in tqdm(os.listdir(os.getcwd())):\n",
    "                  filepath = os.path.join(ROOT_DIR, DIR, CLASS_DIR, filename)\n",
    "                  if filename.endswith(\".jpg\"):\n",
    "                    if DIR == 'train':\n",
    "                      train_file.write(filepath)\n",
    "                      train_file.write(\"\\n\")\n",
    "                    if DIR == 'validation':\n",
    "                      validation_file.write(filepath)\n",
    "                      validation_file.write(\"\\n\")\n",
    "                    if DIR == 'test':\n",
    "                      test_file.write(filepath)\n",
    "                      test_file.write(\"\\n\")\n",
    "                \n",
    "                # Step into Label folder where annotations are generated\n",
    "                os.chdir(\"Label\")\n",
    "\n",
    "                for filename in tqdm(os.listdir(os.getcwd())):\n",
    "                    filename_str = str.split(filename, \".\")[0]\n",
    "                    if filename.endswith(\".txt\"):\n",
    "                        annotations = []\n",
    "                        with open(filename) as f:\n",
    "                            for line in f:\n",
    "                                for class_type in classes:\n",
    "                                    line = line.replace(class_type, str(classes.get(class_type)))\n",
    "                                labels = line.split()\n",
    "                                coords = np.asarray([float(labels[1]), float(labels[2]), float(labels[3]), float(labels[4])])\n",
    "                                coords = convert(filename_str, coords)\n",
    "                                labels[1], labels[2], labels[3], labels[4] = coords[0], coords[1], coords[2], coords[3]\n",
    "                                newline = str(labels[0]) + \" \" + str(labels[1]) + \" \" + str(labels[2]) + \" \" + str(labels[3]) + \" \" + str(labels[4])\n",
    "                                line = line.replace(line, newline)\n",
    "                                annotations.append(line)\n",
    "                            f.close()\n",
    "                        os.chdir(\"..\")\n",
    "                        with open(filename, \"w\") as outfile:\n",
    "                            for line in annotations:\n",
    "                                outfile.write(line)\n",
    "                                outfile.write(\"\\n\")\n",
    "                            outfile.close()\n",
    "                        os.chdir(\"Label\")\n",
    "                os.chdir(\"..\")\n",
    "                os.chdir(\"..\")\n",
    "        os.chdir(\"..\")\n",
    "train_file.close()\n",
    "validation_file.close()\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to folder OIDv4_ToolKit\n",
    "%cd /content/OIDv4_ToolKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move OpenImages Dataset to Yolov4 darknet Dataset Folder\n",
    "DataSet_DIR = os.path.join(os.getcwd(), 'OID', 'Dataset')\n",
    "Dest_DIR = os.path.join('/content', 'darknet', 'data/coco/obj')\n",
    "train_file = open(os.path.join('/content', 'darknet', 'data/coco', 'train.txt'),'a')\n",
    "valid_file = open(os.path.join('/content', 'darknet', 'data/coco', 'valid.txt'),'a')\n",
    "\n",
    "# get images and write the path in Train.txt and Valid.txt\n",
    "# move images and annotation files to Yolov4 Dataset folder\n",
    "def move_dataset(from_dir, to_dir, file_handle):\n",
    "  for filename in tqdm(os.listdir(from_dir)):\n",
    "    print(filename)\n",
    "    if filename.endswith('.txt') or filename.endswith('.jpg'):\n",
    "      if filename.endswith('.jpg'):\n",
    "        file_handle.write(os.path.join(to_dir, filename))\n",
    "        file_handle.write('\\n')\n",
    "      os.rename(os.path.join(from_dir, filename), os.path.join(to_dir, filename))\n",
    "Train_DIR = os.path.join(DataSet_DIR, 'train', 'Person_Car_Bus_Truck_Vehicle registration plate_Human face')\n",
    "Valid_DIR = os.path.join(DataSet_DIR, 'validation', 'Person_Car_Bus_Truck_Vehicle registration plate_Human face')\n",
    "move_dataset(Train_DIR, Dest_DIR, train_file)\n",
    "move_dataset(Valid_DIR, Dest_DIR, valid_file)\n",
    "train_file.close()\n",
    "valid_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Write Custom Configuration for OpenImages Dataset\n",
    "<br>\n",
    "<font size=5>This step is the similar step with custom configuration for COCO</font><br>\n",
    "&emsp;&emsp;cfg/yolov4-custom.cfg<br>\n",
    "&emsp;&emsp;cfg/coco.data<br>\n",
    "&emsp;&emsp;data/coco.names\n",
    "\n",
    "<font size=4>Modify data/coco.names with our own categories</font><br>\n",
    "&emsp;&emsp;Person<br>\n",
    "&emsp;&emsp;Car<br>\n",
    "&emsp;&emsp;Bus<br>\n",
    "&emsp;&emsp;Truck<br>\n",
    "&emsp;&emsp;License Plate<br>\n",
    "&emsp;&emsp;Face<br>\n",
    "\n",
    "<font size=4>Modify cfg/coco.data</font><br>\n",
    "&emsp;&emsp;classes= 6<br>\n",
    "&emsp;&emsp;train  = path/to/train.txt<br>\n",
    "&emsp;&emsp;valid  = path/to/valid.txt<br>\n",
    "&emsp;&emsp;names = /content/darknet/data/coco.names<br>\n",
    "&emsp;&emsp;backup = /content/darknet/backup<br>\n",
    "<br>\n",
    "&emsp;train.txt, valid will be from /content/OIDv4_ToolKit/OID/Dataset folder\n",
    "\n",
    "<font size=4>Modify cfg/yolov4-custom.cfg</font><br>\n",
    "line 20, max_batches = 12000(6*2000)<br>\n",
    "&emsp;&emsp;line 22, steps = 9600, 10800(0.8, 0.9*8000)<br>\n",
    "&emsp;&emsp;change yolo layer classes to 6(class number), line 970, 1058, 1146<br>\n",
    "&emsp;&emsp;change filters of convolution to 33((classes + 5)x3) immediately before each 3 yolo layers, line 963, 1051, 1139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Train the Model with COCO and VOC Dataset\n",
    "<br>\n",
    "This step is the same as # step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
