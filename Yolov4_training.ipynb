{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "Windows or Linux<br>\n",
    "CMake >= 3.12: https://cmake.org/download/<br>\n",
    "CUDA >= 10.0: https://developer.nvidia.com/cuda-toolkit-archive (on Linux do Post-installation Actions)<br>\n",
    "OpenCV >= 2.4: use your preferred package manager (brew, apt), build from source using vcpkg or download from OpenCV official site (on Windows set system variable OpenCV_DIR = C:\\opencv\\build - where are the include and x64 folders image)\n",
    "cuDNN >= 7.0 https://developer.nvidia.com/rdp/cudnn-archive (on Linux copy cudnn.h,libcudnn.so... as desribed here \n",
    "https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar , on Windows copy cudnn.h,cudnn64_7.dll, cudnn64_7.lib as desribed here https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows )<br>\n",
    "GPU with CC >= 3.0: https://en.wikipedia.org/wiki/CUDA#GPUs_supported<br>\n",
    "on Linux GCC or Clang, on Windows MSVC 2017/2019 https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Cloning and Building Darknet\n",
    "The following cells will clone darknet from AlexeyAB's famous repository, adjust the Makefile to enable OPENCV and GPU for darknet and then build darknet.\n",
    "\n",
    "Do not worry about any warnings when you run the '!make' cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "Built on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018\n",
      "Cuda compilation tools, release 10.0, V10.0.130\n"
     ]
    }
   ],
   "source": [
    "# verify CUDA\n",
    "# !nvcc --version\n",
    "!/usr/local/cuda/bin/nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone Yolov4 darknet repo\n",
    "!git clone https://github.com/AlexeyAB/darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change makefile to have GPU and OPENCV enabled\n",
    "%cd darknet\n",
    "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
    "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
    "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
    "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make darknet (builds darknet so that you can then use the darknet executable file to run or train object detectors)\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Download pre-trained YOLOv4 weights\n",
    "YOLOv4 has been trained already on the coco dataset which has 80 classes that it can predict. We will grab these pretrained weights so that we can run YOLOv4 on these pretrained classes and get detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Download COCO Dataset of special categories\n",
    "categories are person, car, bus, truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import io\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2020-12-04 12:15:38--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜images.cocodataset.orgâ€™\n"
     ]
    }
   ],
   "source": [
    "# download coco dataset annotation zip file\n",
    "%cd data\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip annotation zip file\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir coco\n",
    "!mkdir coco/obj\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current directory\n",
    "cdir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(cdir, 'coco/obj')\n",
    "coco_path = os.path.join(cdir, 'coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COCO Bounding Box to Yolov4 Format\n",
    "# COCO Bounding Box format : x(left), y(top), width, height of object\n",
    "# [477.88, 87.8, 13.7, 33.4]\n",
    "# Yolov4 Format: central x, y of object, width, height of object\n",
    "# [0.757391, 0.244731, 0.021406, 0.07822]\n",
    "# rounding 6 decimal points\n",
    "def convertBbox2YoloFormat(bbox, size):\n",
    "  width, height = size\n",
    "  x = round((bbox[0] + bbox[2]/2) / width, 6)\n",
    "  y = round((bbox[1] + bbox[3]/2) / height, 6)\n",
    "  w = round(bbox[2]/width, 6)\n",
    "  h = round(bbox[3]/height, 6)\n",
    "  return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = \"train.txt\"\n",
    "train_txt_path = os.path.join(coco_path, train_txt)\n",
    "valid_txt = \"valid.txt\"\n",
    "valid_txt_path = os.path.join(coco_path, valid_txt)\n",
    "trainfile = open(train_txt_path, 'w')\n",
    "validfile = open(valid_txt_path, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildCustomDatasetFromCOCO(annFile, description_file):\n",
    "    coco=COCO(annFile)\n",
    "    # get Category Ids\n",
    "    catNames = ['person', 'car', 'bus', 'truck']\n",
    "    catIds = coco.getCatIds(catNms=catNames)\n",
    "    # get Image Ids for 4 categories\n",
    "    imgIds = []\n",
    "    for catId in catIds:\n",
    "      sub_imgIds = coco.getImgIds(catIds=catId)\n",
    "      print(len(sub_imgIds))\n",
    "      imgIds += sub_imgIds\n",
    "    # get unique Image Ids\n",
    "    imgIds = list(set(imgIds))\n",
    "    \n",
    "    # loop Image Ids\n",
    "    # get Image Information form Coco dataset\n",
    "    for imgId in imgIds:\n",
    "        img_info = coco.loadImgs(ids = imgId)[0]\n",
    "        # Load Image and annotation\n",
    "        img = io.imread(img_info['coco_url'])\n",
    "        annIds = coco.getAnnIds(imgIds=img_info['id'], catIds=catIds, iscrowd=0)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "\n",
    "        # get file name, e.g. 000000262145.jpg\n",
    "        filename = img_info['file_name']\n",
    "\n",
    "        basename = os.path.splitext(filename)[0]\n",
    "        txtfile_path = os.path.join(dataset_path, basename + '.txt')\n",
    "        # write the image path in train.txt\n",
    "        # example\n",
    "        # coco/obj/000000262145.jpg\n",
    "        # coco/obj/000000262146.jpg\n",
    "        image_path = os.path.join(dataset_path, filename)\n",
    "        description_file.write(image_path + \"\\n\")\n",
    "\n",
    "        # download image in coco/obj folder\n",
    "        io.imsave(image_path, img)\n",
    "\n",
    "        # write the yolo format bounding box in image.txt file\n",
    "        size = (img_info['width'], img_info['height'])\n",
    "        txtfile = open(txtfile_path, 'w')\n",
    "        for i, ann in enumerate(anns):      \n",
    "          bbox = convertBbox2YoloFormat(ann['bbox'], size)\n",
    "          item_str = str(catIds.index(ann['category_id']))\n",
    "          bbox_str = \" \".join(str(entry) for entry in bbox)\n",
    "          item_str += \" \" + bbox_str\n",
    "          if i != len(anns) - 1:\n",
    "            txtfile.write(item_str + \"\\n\")\n",
    "          else:\n",
    "            txtfile.write(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize COCO api for instance annotations\n",
    "dataDir='.'\n",
    "dataType='train2017'\n",
    "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "print(annFile)\n",
    "BuildCustomDatasetFromCOCO(annFile, trainfile)\n",
    "dataType='val2017'\n",
    "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "BuildCustomDatasetFromCOCO(annFile, validfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Write Custom Training Config for YOLOv4\n",
    "cfg/yolov4-custom.cfg<br>\n",
    "cfg/coco.data<br>\n",
    "data/coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify data/coco.names with our own categories\n",
    "person\n",
    "car\n",
    "bus\n",
    "truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify cfg/coco.data\n",
    "classes= 4\n",
    "train  = data/coco/train.txt\n",
    "valid  = data/coco/valid.txt\n",
    "names = data/coco.names\n",
    "backup = backup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify cfg/yolov4-custom.cfg\n",
    "# line 20, max_batches = 8000(4*2000)\n",
    "# line 22, steps = 6400, 7200(0.8, 0.9*8000)\n",
    "# change yolo layer classes to 4(class number), line 970, 1058, 1146\n",
    "# change filters of convolution to 27((classes + 5)x3) immediately before each 3 yolo layers, line 963, 1051, 1139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train the Model with Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "./darknet detector train cfg/coco.data cfg/yolov4-custom.cfg yolov4.conv.137\n",
    "# train with multiple GPU\n",
    "# ./darknet detector train cfg/coco.data cfg/yolov4-custom.cfg yolov4.conv.137 -gpus 0,1,2,3\n",
    "# If want to stop and restart training from a checkpoint:\n",
    "# ./darknet detector train cfg/coco.data cfg/yolov4-custom.cfg backup/yolov3.backup -gpus 0,1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:Infer Custom Objects with Saved YOLOv4 Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define utility function\n",
    "def imShow(path):\n",
    "  import matplotlib.pyplot as plt\n",
    "  %matplotlib inline\n",
    "\n",
    "  image = cv2.imread(path)\n",
    "  height, width = image.shape[:2]\n",
    "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "  fig = plt.gcf()\n",
    "  fig.set_size_inches(18, 10)\n",
    "  plt.axis(\"off\")\n",
    "  #plt.rcParams['figure.figsize'] = [10, 5]\n",
    "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the images with trained yolov4 model\n",
    "#test out our detector!\n",
    "img_path = 'test.jpg'\n",
    "!./darknet detect cfg/yolov4-custom.cfg backup/custom-yolov4-detector_last.weights {img_path} -dont-show\n",
    "imShow('predictions.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7:Download VOC Dataset of special categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download VOC 2007, 2012 dataset tar files from url\n",
    "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
    "!wget http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upzip tar file\n",
    "!tar xf VOCtrainval_06-Nov-2007.tar\n",
    "!tar xf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from os import listdir, getcwd\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset description and classes\n",
    "# in VOC Dataset, there isn't a class of truck, so we can use only 3 classes - person, car, bus\n",
    "sets=[('2012', 'train'), ('2012', 'val'), ('2007', 'train'), ('2007', 'val')]\n",
    "classes = [\"person\", \"car\", , \"bus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert VOC bounding box to yolo darknet format\n",
    "def ConvertVOCBbox2YoloFormat(box, size):\n",
    "    # get central point of object\n",
    "    x = (box[0] + box[1])/2.0\n",
    "    y = (box[2] + box[3])/2.0\n",
    "    # get width and height\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    dw = 1./size[0]\n",
    "    dh = 1./size[1]\n",
    "    # convert bbox to darknet format\n",
    "    x = x * dw\n",
    "    y = y * dh\n",
    "    w = w * dw\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Annotation Xml files from VOC Dataset and create the Yolo darkent text files\n",
    "def ConvertVOCAnns(year, image_id):\n",
    "    # read annotation xml file\n",
    "    in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id))\n",
    "    out_file = open('VOCdevkit/VOC%s/labels/%s.txt'%(year, image_id), 'w')\n",
    "    # parse xml file\n",
    "    root = ET.parse(in_file)\n",
    "    # get size attribute\n",
    "    size = root.find('size')\n",
    "    # get width and height from size attrib\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "    size = (w, h)\n",
    "    # loop object attributes\n",
    "    for obj in root.iter('object'):\n",
    "        # get difficult and class\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        # filter object by class and difficult\n",
    "        if cls not in classes or int(difficult) == 1:\n",
    "            continue\n",
    "        # get class id and bound box\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        xmin = float(xmlbox.find('xmin').text)\n",
    "        ymin = float(xmlbox.find('ymin').text)\n",
    "        xmax = float(xmlbox.find('xmax').text)\n",
    "        ymax = float(xmlbox.find('ymax').text)\n",
    "        box = (xmin, xmax, ymin, ymax)\n",
    "        # convert bound box to darknet format\n",
    "        bbox = ConvertVOCBbox2YoloFormat(box, size)\n",
    "        # make darknet format annotation item\n",
    "        item_str = str(cls_id)\n",
    "        item_str += \" \" + \" \".join([str(e) for e in bbox])\n",
    "        item_str += \"\\n\"\n",
    "        # write item in annotation file\n",
    "        out_file.write(item_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current directory\n",
    "cdir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop all datasets\n",
    "# create the text and valid text file including image paths\n",
    "# this stage is the same as COCO dataset\n",
    "for year, image_set in sets:\n",
    "    if not os.path.exists('VOCdevkit/VOC%s/labels/'%(year)):\n",
    "        os.makedirs('VOCdevkit/VOC%s/labels/'%(year))\n",
    "    image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()\n",
    "    list_file = open('%s_%s.txt'%(year, image_set), 'w')                        \n",
    "    for image_id in image_ids:\n",
    "        list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\\n'%(cdir, year, image_id))\n",
    "        ConvertVOCAnns(year, image_id)\n",
    "    list_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Write Custom Configuration for COCO and VOC Dataset\n",
    "<br>\n",
    "If you need to train with COCO and VOC Dataset, we don't need to modify following files again<br>\n",
    "&emsp;cfg/yolov4-custom.cfg<br>\n",
    "&emsp;cfg/coco.data<br>\n",
    "&emsp;data/coco.names<br>\n",
    "But you need to concatenate the train.txt and valid.txt files of COCO and VOC dataset.<br>\n",
    "And you can move the image files from VOC into the directory of COCO image files - in our case, darknet/data/obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Train the Model with COCO and VOC Dataset\n",
    "<br>\n",
    "This step is the same as # step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
